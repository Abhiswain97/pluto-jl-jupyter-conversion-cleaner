{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A Pluto.jl notebook ###\n",
    "# v0.11.10\n",
    "\n",
    "using Markdown\n",
    "using InteractiveUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\tusing Random\n",
    "\tusing Plots\n",
    "\tusing HDF5\n",
    "\tusing Statistics\n",
    "\tusing Base.Iterators\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey! This post is about my introduction to the world of Julia. I took this challenge of learning Julia and making something in it. Since Julia is pretty similar to Python, I made a hypothesis. That is can I learn julia and be up and running with something in two days? What I realised is, if you're from a python background and have some expereince in it, then learning Julia is going to be fun and breezy for you. So, here I am after my two day rendezvous with Julia.\n",
    "\n",
    "So, what I used to learn Julia?\n",
    "I used resources from [julia academy](https://juliaacademy.com/)\n",
    "\n",
    "What did I implement?\n",
    "I decided to go for one the resources I learnt deep learning from: [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome)\n",
    "\n",
    "Impemented the Julia version of [Week 2 assignment](https://www.coursera.org/learn/neural-networks-deep-learning/notebook/zAgPl/logistic-regression-with-a-neural-network-mindset) of [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome) course.\n",
    "\n",
    "I hope it's useful to you. It was a lot of fun and I am in love with Julia â¤\n",
    "\n",
    "Let's begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "1. There are two files: `train_catvnoncat.h5` & `test_catvnoncat.h5`\n",
    "2. According to our notation, `X` is of shape *(num_features, num_examples)* & `y` is a row vector of shape *(1, num_examples)*. \n",
    "3. We write a function `load_dataset()` which:\n",
    "    - Takes in HDF5 files\n",
    "    - Converts them into `Array{Float64, 2}` arrays.\n",
    "    - Reshapes them according to our notation & returns `X_train, y_train, X_test, y_test`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_dataset(train_file::String, test_file::String)\n",
    "    \n",
    "    X_train = convert(Array{Float64, 4}, h5read(train_file, \"train_set_x\"))\n",
    "    y_train = convert(Array{Float64, 1}, h5read(train_file, \"train_set_y\"))\n",
    "    \n",
    "    X_test = convert(Array{Float64, 4}, h5read(test_file, \"test_set_x\"))\n",
    "    y_test = convert(Array{Float64, 1}, h5read(test_file, \"test_set_y\"))\n",
    "    \n",
    "    num_features_train_X = size(X_train, 1) * size(X_train, 2) * size(X_train, 2)\n",
    "    num_features_test_X = size(X_test, 1) * size(X_test, 2) * size(X_test, 2)\n",
    "    \n",
    "    X_train = reshape(X_train, (num_features_train_X, size(X_train, 4)))\n",
    "    y_train = reshape(y_train, (1, size(y_train, 1)))\n",
    "    \n",
    "    X_test = reshape(X_test, (num_features_test_X, size(X_test, 4)))\n",
    "    y_test = reshape(y_test, (1, size(y_test, 1)))\n",
    "    \n",
    "    X_train, y_train, X_test, y_test\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\tX_train, y_train, X_test, y_test = load_dataset(\n",
    "\t\traw\"C:\\Users\\Abhishek Swain\\Desktop\\Neural-Networks-and-Deep-Learning-in-Julia\\Week-2\\train_catvnoncat.h5\", \n",
    "\t\traw\"C:\\Users\\Abhishek Swain\\Desktop\\Neural-Networks-and-Deep-Learning-in-Julia\\Week-2\\test_catvnoncat.h5\");\n",
    "\n",
    "\tX_train, X_test= X_train/255, X_test/255;\n",
    "\t\n",
    "\t@time size(X_train), size(y_train), size(X_test), size(y_test)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "Applies sigmoid to the vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Ïƒ(z) \n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    \"\"\"\n",
    "    return one(z) / (one(z) + exp(-z))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialization\n",
    "Initialize `w` & `b` with with random values between (0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize(dim)\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = zeros(dim, 1)\n",
    "    b = 2\n",
    "    \n",
    "    @assert(size(w) == (dim, 1))\n",
    "    @assert(isa(b, Float64) || isa(b, Int64))\n",
    "    \n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function propagate(w, b, X, Y)\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation\n",
    "    \"\"\"\n",
    "    m = size(X, 2)\n",
    "    \n",
    "    # Forward prop\n",
    "    Z = w'X .+ b\n",
    "    yÌ‚ = Ïƒ.(Z)\n",
    "    \n",
    "    @assert(size(yÌ‚) == size(Y))\n",
    "    \n",
    "    # Compute cost\n",
    "    ð’¥ = -1 * sum(Y .* log.(yÌ‚) .+ (1 .- Y) .* log.(1 .- yÌ‚))\n",
    "    ð’¥ /= m\n",
    "    \n",
    "    @assert(size(ð’¥) == ())\n",
    "    \n",
    "    # Back-prop\n",
    "    ðœ•ð‘§ = yÌ‚ - Y\n",
    "    @assert(size(ðœ•ð‘§) == size(yÌ‚) && size(ðœ•ð‘§) == size(Y))\n",
    "        \n",
    "    ðœ•ð‘¤ = (1/m) * X * ðœ•ð‘§'\n",
    "    ðœ•ð‘ = (1/m) * sum(ðœ•ð‘§)\n",
    "    \n",
    "    ð’¥, Dict(\"ðœ•ð‘¤\" => ðœ•ð‘¤, \"ðœ•ð‘\" => ðœ•ð‘)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize(w, b, X, Y, num_iterations, ð›¼, print_cost)\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = Array{Float64, 2}(undef, num_iterations, 1)\n",
    "    \n",
    "    for i=1:num_iterations\n",
    "        \n",
    "        ð’¥, ð›» = propagate(w, b, X, Y)\n",
    "        \n",
    "        ðœ•ð‘¤, ðœ•ð‘ = ð›»[\"ðœ•ð‘¤\"], ð›»[\"ðœ•ð‘\"] \n",
    "        \n",
    "        global ðœ•ð‘¤, ðœ•ð‘\n",
    "        \n",
    "        w -= ð›¼ .* ðœ•ð‘¤\n",
    "        b -= ð›¼ .* ðœ•ð‘\n",
    "        \n",
    "        costs[i] = ð’¥\n",
    "        \n",
    "        if print_cost && i % 100 == 0\n",
    "            println(\"Cost after iteration $i = $ð’¥\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    params = Dict(\"w\" => w, \"b\" => b)\n",
    "    grads = Dict(\"ðœ•ð‘¤\" => ðœ•ð‘¤, \"ðœ•ð‘\" => ðœ•ð‘)\n",
    "    \n",
    "    params, grads, costs\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(w, b, X)\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    \"\"\"\n",
    "    m = size(X, 2)\n",
    "    preds = zeros(1, m)\n",
    "    \n",
    "    yÌ‚ = Ïƒ.(w'X .+ b)\n",
    "    \n",
    "    preds = [p > 0.5 ? 1 : 0 for p in Iterators.flatten(yÌ‚)]\n",
    "    \n",
    "    preds = reshape(preds, (1, m))\n",
    "            \n",
    "    @assert(size(preds) == (1, m))\n",
    "    \n",
    "    preds\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Combine all functions to train the model.\n",
    "Learning rate: $\\alpha = 0.005$, iterations(epochs): 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function model(X_train, y_train, X_test, y_test, num_iterations, ð›¼, print_cost)\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    w, b = initialize(size(X_train, 1))\n",
    "    \n",
    "    # Gradient descent\n",
    "    params, grads, costs = optimize(w, b, X_train, y_train, num_iterations, ð›¼, print_cost)\n",
    "    \n",
    "    w, b = params[\"w\"], params[\"b\"]\n",
    "    \n",
    "    preds_test = predict(w, b, X_test)\n",
    "    preds_train = predict(w, b, X_train)\n",
    "    \n",
    "    train_acc = 100 - mean(abs.(preds_train - y_train)) * 100\n",
    "    test_acc = 100 - mean(abs.(preds_test - y_test)) * 100\n",
    "    \n",
    "    @show train_acc\n",
    "    @show test_acc\n",
    "    \n",
    "    d = Dict(\n",
    "        \"costs\" => costs, \n",
    "        \"test_preds\" => preds_test, \n",
    "        \"train_preds\" => preds_train,\n",
    "        \"w\" => w,\n",
    "        \"b\" => b,\n",
    "        \"ð›¼\" => ð›¼,\n",
    "        \"num_iterations\" =>  num_iterations\n",
    "    )\n",
    "    \n",
    "    d;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(X_train, y_train, X_test, y_test, 2000, 0.005, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\tx = 1:2000;\n",
    "\ty = d[\"costs\"];\n",
    "\n",
    "\tgr() # backend\n",
    "\n",
    "\tplot(x, y, title = \"Learning rate = 0.005\", label=\"negative log-likelihood\")\n",
    "\txlabel!(\"iteration\")\n",
    "\tylabel!(\"cost\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
